{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf ## Use Tensorflow Version 1\n",
    "import tflib as lib\n",
    "import tflib.plot\n",
    "import tflib.cifar10_fed\n",
    "import tflib.sn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CIFAR-10 (Python version) at\n",
    "# https://www.cs.toronto.edu/~kriz/cifar.html and fill in the path to the\n",
    "# extracted files here!\n",
    "DATA_DIR = 'Cifar10'\n",
    "if len(DATA_DIR) == 0:\n",
    "    raise Exception('Please specify path to data directory in gan_cifar.py!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10 # Batch size\n",
    "TEST_BATCH_SIZE = 1000\n",
    "Sample_size= 50000\n",
    "ITERS = 10000\n",
    "INPUT_DIM = 3*32*32 # Number of pixels in CIFAR\n",
    "nodes = 100\n",
    "maximize_iters = 10\n",
    "test_iters = 100\n",
    "noise_std = 5.0\n",
    "tau=1\n",
    "\n",
    "address = 'cifar_fedOT_inception'+'_samplesize_'+str(Sample_size)+'_nodes_'+str(nodes)+'noise_constant_var_'+str(noise_std)\n",
    "\n",
    "if not os.path.exists(address):\n",
    "    os.makedirs(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(input_data, num_classes=10, wd=0, update_collection=None, beta=1., reuse=None, training=False,num=1):\n",
    "    \"\"\"AlexNet architecture\n",
    "        two [convolution 5x5 -> max-pool 3x3 -> local-response-normalization] modules \n",
    "        followed by two fully connected layers with 384 and 192 hidden units, respectively. \n",
    "        Finally a NUM_CLASSES-way linear layer is used for prediction\n",
    "    \"\"\"\n",
    "    input_data_reshaped = tf.reshape(input_data,[-1,32,32,3])\n",
    "    conv = sn.conv2d(input_data_reshaped, [5, 5, 3, 96], scope_name='conv1'+'_num_'+str(num), spectral_norm=False, reuse=reuse)\n",
    "    conv1 = tf.nn.relu(conv, name='conv1_relu'+'_num_'+str(num))\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='VALID', name='pool1'+'_num_'+str(num))\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1'+'_num_'+str(num))\n",
    "    \n",
    "    conv = sn.conv2d(norm1, [5, 5, 96, 256], scope_name='conv2'+'_num_'+str(num), spectral_norm=False, reuse=reuse)\n",
    "    conv2 = tf.nn.relu(conv, name='conv2_relu'+'_num_'+str(num))\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='VALID', name='pool2'+'_num_'+str(num))\n",
    "    norm2 =  tf.nn.lrn(pool2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2'+'_num_'+str(num))\n",
    "    \n",
    "    reshape = tf.reshape(norm2, [-1, 7*7*256])\n",
    "    lin = sn.linear(reshape, 384, scope_name='linear1'+'_num_'+str(num), spectral_norm=False, reuse=reuse)\n",
    "    lin1 = tf.nn.relu(lin, name='linear1_relu'+'_num_'+str(num))\n",
    "\n",
    "    lin = sn.linear(lin1, 192, scope_name='linear2'+'_num_'+str(num), spectral_norm=False, reuse=reuse)\n",
    "    lin2 = tf.nn.relu(lin, name='linear2_relu'+'_num_'+str(num))\n",
    "\n",
    "    fc = sn.linear(lin2, num_classes, scope_name='fc'+'_num_'+str(num), spectral_norm=False, reuse=reuse)\n",
    "        \n",
    "    return fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Tensor(\"Reshape_201:0\", shape=(1000, 10), dtype=float32)\n",
      "WARNING:tensorflow:From /state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:2403: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "LAMBDA_0 = 1.0\n",
    "LAMBDA_1 = 10.0\n",
    "adv_stepsize = 2.0\n",
    "\n",
    "real_data_int = tf.placeholder(tf.int32, shape=[BATCH_SIZE*nodes, INPUT_DIM])\n",
    "real_data = 2*((tf.cast(real_data_int, tf.float32)/255.)-.5)\n",
    "theta=tf.Variable(tf.zeros(shape=[nodes, INPUT_DIM],dtype=tf.float32),dtype=tf.float32,name='Theta')\n",
    "theta_1=tf.Variable(tf.ones(shape=[nodes, INPUT_DIM],dtype=tf.float32),dtype=tf.float32,name='Theta_quad')\n",
    "maxVar = tf.Variable(tf.zeros(shape=[nodes, INPUT_DIM],dtype=tf.float32),dtype=tf.float32,name='maxVar')\n",
    "maxVar_1 = tf.Variable(tf.zeros(shape=[nodes, INPUT_DIM],dtype=tf.float32),dtype=tf.float32,name='maxVar_quad')\n",
    "label = tf.placeholder(tf.int64, shape=[BATCH_SIZE*nodes])\n",
    "\n",
    "data_perturbed_list = []\n",
    "data_perturbed_list_max = []\n",
    "for i in range(nodes):\n",
    "    data_perturbed_list.append( tf.multiply(real_data[i*BATCH_SIZE:(i+1)*BATCH_SIZE,:],theta_1[i,:])+theta[i,:])\n",
    "\n",
    "data_perturbed = tf.stack(data_perturbed_list)\n",
    "data_perturbed_max = tf.reduce_mean(data_perturbed,reduction_indices=[1])\n",
    "data_perturbed_pow_2_max = tf.reduce_mean(data_perturbed**2,reduction_indices=[1])\n",
    "\n",
    "NN_out_perturbed_list = []\n",
    "for i in range(nodes):\n",
    "    NN_out_perturbed_list.append(alexnet(tf.squeeze(data_perturbed[i,:,:]),num=i)) \n",
    "NN_out_perturbed =  tf.stack(NN_out_perturbed_list)\n",
    "NN_out_perturbed = tf.reshape(NN_out_perturbed,[BATCH_SIZE*nodes, 10])\n",
    "\n",
    "train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(NN_out_perturbed,axis=1),label),dtype=tf.float32))\n",
    "\n",
    "train_loss= tf.reduce_mean( tf.reduce_logsumexp(NN_out_perturbed,reduction_indices=[1])\n",
    "                           - tf.diag_part(tf.gather(NN_out_perturbed,label,axis=1)))\n",
    "max_loss = tf.reduce_sum(tf.multiply(data_perturbed_max,maxVar-tf.reduce_mean(maxVar,reduction_indices=[0]) ) )\n",
    "max_loss_1 = tf.reduce_sum(tf.multiply(data_perturbed_pow_2_max,maxVar_1-tf.reduce_mean(maxVar_1,reduction_indices=[0]) ) )\n",
    "train_loss_2 = (train_loss - LAMBDA_0*tf.reduce_sum(maxVar**2) - LAMBDA_1*tf.reduce_sum(maxVar_1**2) \n",
    "                + adv_stepsize*(max_loss+max_loss_1)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_int = tf.placeholder(tf.int64, shape=[TEST_BATCH_SIZE, INPUT_DIM])\n",
    "valid_data = 2.*((tf.cast(valid_data_int, tf.float32)/255.)-.5)\n",
    "valid_label = tf.placeholder(tf.int64, shape=[TEST_BATCH_SIZE])\n",
    "\n",
    "test_size= int(TEST_BATCH_SIZE/nodes)\n",
    "valid_NN_out_list = []\n",
    "for i in range(nodes):\n",
    "    valid_NN_out_list.append(\n",
    "        alexnet(tf.multiply(valid_data[i*test_size:(i+1)*test_size,:],theta_1[i,:])+theta[i,:],num=i,reuse=True))\n",
    "\n",
    "valid_NN_out = tf.stack(valid_NN_out_list)\n",
    "valid_NN_out = tf.reshape(valid_NN_out,[TEST_BATCH_SIZE,10])\n",
    "\n",
    "valid_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(valid_NN_out,axis=1),valid_label),dtype=tf.float32))\n",
    "valid_loss= tf.reduce_mean( tf.log(tf.reduce_sum(tf.exp(valid_NN_out),reduction_indices=[1]))\n",
    "                          - tf.diag_part(tf.gather(valid_NN_out,valid_label,axis=1))  )\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "params = tf.trainable_variables()\n",
    "nn_vars=[]\n",
    "for i in range(nodes):\n",
    "    nn_vars.append([var for var in params if ('num_'+str(i)) in var.name])\n",
    "nn_params = [var for var in params if (('Theta' in var.name) or (('_num') in var.name))]\n",
    "max_params=[var for var in params if 'maxVar' in var.name]\n",
    "\n",
    "Classifier_train_op = tf.train.GradientDescentOptimizer(\n",
    "        learning_rate=1e-4\n",
    "    ).minimize(train_loss_2, var_list=nn_params)\n",
    "\n",
    "Max_train_op = tf.train.GradientDescentOptimizer(\n",
    "        learning_rate=1e-3\n",
    "    ).minimize(-train_loss_2, var_list=max_params)\n",
    "\n",
    "assign_op = [nn_vars[j][i].assign( (1./nodes)*sum(nn_vars[k][i] for k in range(nodes))) for i in range(len(nn_vars[0])) for j in range(nodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_gen, dev_gen = lib.cifar10_fed.load(BATCH_SIZE, TEST_BATCH_SIZE, data_dir=DATA_DIR, k= nodes,sample_size=Sample_size)\n",
    "def inf_train_gen():\n",
    "    while True:\n",
    "        for elements in train_gen():\n",
    "            for (images,targets) in elements:\n",
    "                yield ((images.reshape((-1,3,32,32))).transpose((0,2,3,1))).reshape((-1,3*32*32)),targets\n",
    "            \n",
    "def inf_test_gen():\n",
    "    while True:\n",
    "        for elements in dev_gen():\n",
    "            for (images,targets) in elements:\n",
    "                yield ((images.reshape((-1,3,32,32))).transpose((0,2,3,1))).reshape((-1,3*32*32)),targets\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss_arr = []\n",
    "train_acc_arr= []\n",
    "train_loss_perturbed_arr = []\n",
    "train_acc_perturbed_arr= []\n",
    "valid_acc_arr = []\n",
    "valid_acc_perturbed_arr = []\n",
    "\n",
    "np.random.seed(1234)\n",
    "perturbation_add_train = noise_std*np.random.normal(size=[nodes,INPUT_DIM])\n",
    "matrix_mult_train = (noise_std/(np.sqrt(INPUT_DIM)))*np.random.normal(size=[nodes,INPUT_DIM,INPUT_DIM])\n",
    "\n",
    "with tf.Session() as session:\n",
    "\n",
    "\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    gen = inf_train_gen()\n",
    "    gen_test = inf_test_gen()\n",
    "    _data_agg = np.zeros([BATCH_SIZE*nodes,INPUT_DIM],dtype=np.float32)\n",
    "    _data_perturbed_agg = np.zeros([BATCH_SIZE*nodes,INPUT_DIM],dtype=np.float32)\n",
    "    _labels_agg = np.zeros([BATCH_SIZE*nodes],dtype=np.int64)\n",
    "    for iteration in range(ITERS):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for k in range(nodes):    \n",
    "            \n",
    "            data_inf = next(gen)\n",
    "            _data = data_inf[0]\n",
    "            _data_agg[k*BATCH_SIZE:(k+1)*BATCH_SIZE,:] = 2.*(_data/255.-0.5)\n",
    "            _data_agg[k*BATCH_SIZE:(k+1)*BATCH_SIZE,:] += np.matmul(_data_agg[k*BATCH_SIZE:(k+1)*BATCH_SIZE,:],\n",
    "                                                                   np.squeeze(matrix_mult_train[k,:,:]))\n",
    "            _data_agg[k*BATCH_SIZE:(k+1)*BATCH_SIZE,:] +=  perturbation_add_train[k,:]\n",
    "            _labels_agg[k*BATCH_SIZE:(k+1)*BATCH_SIZE] = data_inf[1]\n",
    "        \n",
    "        _data = np.int32(127.5*_data_agg+127.5)    \n",
    "        _labels = _labels_agg    \n",
    "        \n",
    "        for _ in range(tau):\n",
    "            for _ in range(maximize_iters):\n",
    "                session.run(Max_train_op,feed_dict={real_data_int: _data,label: _labels})                                                         \n",
    "            session.run(Classifier_train_op,feed_dict={real_data_int: _data,label: _labels})                                                         \n",
    "        \n",
    "        _,_train_loss_perturbed,_train_acc_perturbed = session.run([assign_op,train_loss,train_acc],\n",
    "                                                              feed_dict={real_data_int: _data,label: _labels})                                                         \n",
    "\n",
    "        train_loss_perturbed_arr.append(_train_loss_perturbed)\n",
    "        train_acc_perturbed_arr.append(_train_acc_perturbed)\n",
    "        \n",
    "        # Write logs every 500 iters\n",
    "        \n",
    "        if iteration % 500 == 0:\n",
    "            test_data_inf = next(gen_test)\n",
    "            _data_valid = test_data_inf[0]\n",
    "            _labels_valid = test_data_inf[1]\n",
    "            \n",
    "            test_size= int(TEST_BATCH_SIZE/nodes)\n",
    "            _data_agg = np.zeros([test_size*nodes,INPUT_DIM],dtype=np.float32)\n",
    "            for k in range(nodes):    \n",
    "            \n",
    "                _data = _data_valid[k*test_size:(k+1)*test_size,:]\n",
    "                _data_agg[k*test_size:(k+1)*test_size,:] = 2.*(_data/255.-0.5)\n",
    "                _data_agg[k*test_size:(k+1)*test_size,:] += np.matmul(_data_agg[k*test_size:(k+1)*test_size,:],\n",
    "                                                                       np.squeeze(matrix_mult_train[k,:,:]))\n",
    "                _data_agg[k*test_size:(k+1)*test_size,:] +=  perturbation_add_train[k,:]\n",
    "        \n",
    "            _data_valid = np.int32(127.5*_data_agg+127.5)    \n",
    "\n",
    "\n",
    "            _valid_acc  = session.run(valid_acc, feed_dict={valid_data_int: _data_valid,valid_label:_labels_valid})       \n",
    "            \n",
    "            _valid_acc_perturbed  = session.run(valid_acc, feed_dict={valid_data_int: _data_valid,valid_label:_labels_valid})      \n",
    "\n",
    "            valid_acc_arr.append(_valid_acc)\n",
    "            valid_acc_perturbed_arr.append(_valid_acc_perturbed)\n",
    "            \n",
    "            np.save(address+'/train_loss_perturbed_arr',train_loss_perturbed_arr)\n",
    "            np.save(address+'/train_acc_perturbed_arr',train_acc_perturbed_arr)\n",
    "            np.save(address+'/valid_acc_arr',valid_acc_arr)\n",
    "            np.save(address+'/valid_acc_perturbed_arr',valid_acc_perturbed_arr)\n",
    "\n",
    "            \n",
    "        if iteration % 50 == 0 or iteration<10:\n",
    "            lib.plot.flush()\n",
    "\n",
    "        lib.plot.tick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
